## DeepSeek R1

### Links

* https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf
* https://chat.deepseek.com/sign_in

### Notes

- DeepSeek R1 was released on January 21, 2025.
- DeepSeek R1 claims performance on par with OpenAI's o1 model.
- A technical report for DeepSeek R1 is available.
- The release includes open-source distilled models for research and commercial use.
- DeepSeek R1 is a reasoning model, also known as DeepSeek Thinking.
- The model is accessible via DeepSeek's platform and available as a mobile app.
- DeepSeek R1 is one of the first open-source models to compete directly with OpenAI's o1 model.
- It shows comparable or superior performance on benchmarks such as Codeforces and Code GPQ DI.
- The model demonstrates capabilities in self-verification, reflection, and generating long chains of thought.
- DeepSeek R1 utilizes reinforcement learning without relying on supervised fine-tuning (SFT).
- It applies the GRPO reinforcement learning algorithm.
- The model incorporates a reward system based on accuracy and format to ensure consistent output.
- DeepSeek R1 explores chain-of-thought reasoning to solve complex problems.
- DeepSeek R10 transitions directly from a base model to reinforcement learning.
- DeepSeek R10 shows close performance to OpenAI's o1 model on various benchmarks.
- DeepSeek R10 learns to allocate more thinking time, enhancing reasoning capabilities.
- DeepSeek R1 involves a multi-step training pipeline including reinforcement learning (RL) and supervised fine-tuning (SFT).
- The training process addresses readability and language mixing issues through a multi-training procedure.
- DeepSeek R1 utilizes 600k reasoning-related training examples.
- The model enhances healthfulness and harmlessness while maintaining reasoning abilities.
- DeepSeek R1 aligns with human preferences to ensure usability and task understanding.
- The training pipeline begins with reinforcement learning from a cold start.
- It moves to supervised fine-tuning with rejection sampling.
- The pipeline incorporates reasoning-oriented RL to improve performance on tasks like coding, mathematics, science, and logic.
- A language consistency reward is introduced during RL training to improve output quality.
- DeepSeek R1 successfully understands and executes tasks, improving over previous models in task comprehension.
- The model demonstrated the ability to generate correct code, such as bash scripts for matrix transposition.
- DeepSeek R1 has enhanced reasoning and problem-solving capabilities compared to earlier iterations.
- The model is available for research and commercial purposes.
- A mobile application is available for users to interact with DeepSeek R1.
- Ongoing testing and experimentation aim to further enhance model capabilities.
- Plans include releasing additional tutorials and expanding functionality based on user feedback.
